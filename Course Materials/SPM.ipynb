{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "import smart_open as sm\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path='data/news.txt.gz'):\n",
    "    with sm.open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            cat, headline, text = line.strip().split('\\t')\n",
    "            yield cat, headline, text\n",
    "            \n",
    "        \n",
    "def tokenize_text(text):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "def normalize_text(text):\n",
    "    return ' '.join(tokenize_text(text))\n",
    "\n",
    "def prepare_spm_file(f_out, data):\n",
    "    with open(f_out, 'w', encoding='utf-8') as f:\n",
    "        for cat, headline, text in tqdm_notebook(data):\n",
    "            f.write(normalize_text(headline))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            sents = (sent for sent in re.split(r'[.!?]', text) if len(sent) > 20)\n",
    "    \n",
    "            for sent in sents:\n",
    "                f.write(normalize_text(sent))\n",
    "                f.write('\\n')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'ws/spm.txt' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ws/spm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"ws/spm.txt\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-982bb4e69a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munk_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnews_spm\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                --vocab_size=5000')\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"ws/spm.txt\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=ws/spm.txt \\\n",
    "                               --pad_id=0 \\\n",
    "                               --bos_id=2 \\\n",
    "                               --eos_id=3  \\\n",
    "                               --unk_id=1 \\\n",
    "                               --model_prefix=news_spm \\\n",
    "                               --vocab_size=5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"ws/news_spm.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-75a4458fbcfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ws/news_spm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_Load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadOrDie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"ws/news_spm.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "proc = spm.SentencePieceProcessor()\n",
    "proc.Load('ws/news_spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.EncodeAsPieces('октября')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ws/spm.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6e48804141f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sents = [proc.EncodeAsPieces(line.strip()) \n\u001b[0;32m----> 4\u001b[0;31m                      for line in open('ws/spm.txt', encoding='utf-8')]\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ws/spm.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sents = [proc.EncodeAsPieces(line.strip()) \n",
    "                     for line in open('ws/spm.txt', encoding='utf-8')]\n",
    "w2v = Word2Vec(sent,size=200)\n",
    "\n",
    "w2v.wv.save_word2vec_format('ws/w2v_vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁июля', 0.976055383682251),\n",
       " ('▁августа', 0.9743028283119202),\n",
       " ('▁июня', 0.9722191095352173),\n",
       " ('▁ноября', 0.9719325304031372),\n",
       " ('▁сентября', 0.9707849025726318),\n",
       " ('▁декабря', 0.9706289768218994),\n",
       " ('▁марта', 0.969725489616394),\n",
       " ('▁января', 0.9664162993431091),\n",
       " ('▁апреля', 0.9662265181541443),\n",
       " ('▁февраля', 0.9658150672912598)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(proc.EncodeAsPieces('октября'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = w2v.wv.vector_size\n",
    "\n",
    "def _piece_id_to_vect(piece_id):\n",
    "    piece = proc.id_to_piece(piece_id) \n",
    "    if piece in w2v.wv:\n",
    "        return w2v.wv[piece]\n",
    "    return np.zeros((emb_size,))\n",
    "\n",
    "emb = np.array([_piece_id_to_vect(piece_id) for piece_id in range(0, len(proc))])\n",
    "np.save('ws/vectors.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁при', 'вет', '▁мир']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.EncodeAsPieces(normalize_text('привет мир'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted({label for (label, _, _) in read_data()})\n",
    "label_to_idx = {label:idx for (idx, label) in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 120\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = normalize_text(text)\n",
    "    pieces = proc.EncodeAsIds(text)\n",
    "    if len(pieces) > max_seq_len:\n",
    "        pieces = pieces[:max_seq_len]\n",
    "    to_add = (max_seq_len - len(pieces))\n",
    "    pieces = pieces + to_add * [proc.pad_id()]\n",
    "    \n",
    "    return np.array(pieces)\n",
    "    \n",
    "\n",
    "def prepare_data(label_to_idx):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label, headline, text in read_data():\n",
    "        label_id = label_to_idx[label]\n",
    "        X.append(prepare_text(headline + ' ' + text))\n",
    "        y.append(label_id)\n",
    "       \n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "X, y = prepare_data(label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2508,  0.1793,  0.2189,  ..., -1.1806,  2.7978, -0.2542],\n",
       "        [-1.0755,  0.7262,  0.5747,  ..., -0.4297,  0.7703,  0.1982],\n",
       "        [ 0.4433,  0.6389, -0.5718,  ..., -0.0887,  0.7056, -0.2806],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer = nn.Embedding.from_pretrained(torch.tensor(emb), padding_idx=proc.pad_id())\n",
    "emb_layer(torch.tensor(prepare_text('привет мир')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.LongTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "\n",
    "l = X.size(0)\n",
    "l_train, l_test = int(l * 0.7), int(l * 0.2)\n",
    "\n",
    "data = TensorDataset(X, y)\n",
    "train_ds, test_ds, val_ds = random_split(data, [l_train, l_test, l - l_train - l_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.EmbeddingBag.from_pretrained(torch.FloatTensor(emb)),\n",
    "                      nn.Linear(emb.shape[1], 20),\n",
    "                      #nn.ReLU(),\n",
    "                      #nn.Dropout(0.2),\n",
    "                      nn.Linear(20, len(labels)),\n",
    "                      nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, acc = 0.46157, loss = 1.9048439264297485\n",
      "Epoch = 10, acc = 0.64229, loss = 1.7881412506103516\n",
      "Epoch = 20, acc = 0.65129, loss = 1.7816226482391357\n",
      "Epoch = 30, acc = 0.66000, loss = 1.7215139865875244\n",
      "Epoch = 40, acc = 0.66343, loss = 1.7081806659698486\n",
      "Epoch = 50, acc = 0.65986, loss = 1.6618688106536865\n",
      "Epoch = 60, acc = 0.66571, loss = 1.6168361902236938\n",
      "Epoch = 70, acc = 0.66300, loss = 1.5674864053726196\n",
      "Epoch = 80, acc = 0.66086, loss = 1.663656234741211\n",
      "Epoch = 90, acc = 0.67014, loss = 1.736398696899414\n",
      "Epoch = 100, acc = 0.66729, loss = 1.5614030361175537\n",
      "Epoch = 110, acc = 0.67100, loss = 1.5611494779586792\n",
      "Epoch = 120, acc = 0.66729, loss = 1.561150312423706\n",
      "Epoch = 130, acc = 0.67286, loss = 1.5692418813705444\n",
      "Epoch = 140, acc = 0.66514, loss = 1.5646708011627197\n",
      "Epoch = 150, acc = 0.67557, loss = 1.561161994934082\n",
      "Epoch = 160, acc = 0.67186, loss = 1.5611525774002075\n",
      "Epoch = 170, acc = 0.66986, loss = 1.5611501932144165\n",
      "Epoch = 180, acc = 0.67357, loss = 1.5617300271987915\n",
      "Epoch = 190, acc = 0.67957, loss = 1.5698127746582031\n",
      "Epoch = 200, acc = 0.67571, loss = 1.7058162689208984\n",
      "Epoch = 210, acc = 0.68086, loss = 1.5611661672592163\n",
      "Epoch = 220, acc = 0.65814, loss = 1.5611501932144165\n",
      "Epoch = 230, acc = 0.67186, loss = 1.5611501932144165\n",
      "Epoch = 240, acc = 0.67829, loss = 1.561150312423706\n",
      "Epoch = 250, acc = 0.67257, loss = 1.5611501932144165\n",
      "Epoch = 260, acc = 0.66429, loss = 1.5611501932144165\n",
      "Epoch = 270, acc = 0.66000, loss = 1.5611501932144165\n",
      "Epoch = 280, acc = 0.67871, loss = 1.661151647567749\n",
      "Epoch = 290, acc = 0.67186, loss = 1.6074718236923218\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train, val, test, max_epochs=300):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=30)\n",
    "    val_loader = DataLoader(dataset=val_ds)\n",
    "    test_loader = DataLoader(dataset=test_ds)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        cur = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:   \n",
    "            model.train()\n",
    "            y_pred = model(X_batch)    \n",
    "            bce = loss(y_pred, y_batch)\n",
    "                        \n",
    "            bce.backward()        \n",
    "\n",
    "            cur += (y_pred.argmax(1) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()       \n",
    "           \n",
    "        if epoch % 10 == 0:        \n",
    "            acc = cur / total\n",
    "            print(f'Epoch = {epoch}, acc = {acc:.5f}, loss = {bce}')   \n",
    "\n",
    "train_model(model, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
